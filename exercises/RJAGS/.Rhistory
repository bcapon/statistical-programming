NoEffectLMMprof <- function(theta, yXBeta2, n) {
# theta is the log of standard deviation so perform the inverse to get sigma**2:
sigma2 <- exp(2 * theta)
# The log likelihood of the normal linear regression model is - (y-Xβ)^T(y-Xβ)/2σ**2
# - (n/2)ln(σ**2) - (n/2)ln(2π). The last term can be dropped as it's a constant
# so calculate this as loglik and return it:
loglik <- -yXBeta2 / (2 * sigma2) - n * log(sigma2) / 2
return(loglik)
}
## The function LMMprof evaluates the negative log-likelihood of the model for a
## given set of log standard deviations (the vector θ), and stores the corresponding
## estimates of the fixed effects parameters β as an attribute.
##
## Inputs:
## theta: the vector of log-standard deviations of the residuals and random effects.
## y: the response vector.
## X: the design matrix for fixed effects.
## qrZ: the QR decomposition of the random effects design matrix Z.
## lengths: number of random effects variables in each entry of ref.
##
## Outputs:
## loglikelihood: the negative log-likelihood of the model for the given θ, with
## beta: estimate β stored as an attribute.
LMMprof <- function(theta, y, X, qrZ, lengths) {
# Compute the dimensions of n x p random effects design matrix Z.
p <- sum(lengths)
n <- length(y)
# Calculate the variance of the residuals (σ**2) from theta[1] = log(σ).
sigma2 <- exp(2 * theta[1])
# Construct random effects covariance matrix ψ from remaining values in θ where
# ψ is a diagonal matrix; random effects are independent.
# N.B. theta[-1] are the log-standard deviations of the random effects.
psi <- diag(rep(exp(2 * theta[-1]), lengths))
# Combine random effects covariance matrix ψ and residual variance to
# construct reduced covariance matrix of model.
mat2log <- qr.R(qrZ) %*% psi %*% t(qr.R(qrZ)) + diag(sigma2, p)
# Compute Cholesky factorisation of covariance matrix (R^(T)R = S).
MatChol <- chol(mat2log)
# Compute half the logarithm of the determinant of the reduced covariance matrix.
halflogofdet <- sum(log(diag(MatChol))) + (n - p) * log(sigma2) / 2
# Compute W-weighted version of design matrix X which has useful attributes we
# can use.
lis <- Wmul(X, qrZ, sigma2, n, p, psi)
# Extract Cholesky factor L for reuse.
L <- lis$L
# Extract the crossproduct between W and x using our lis variable.
WX <- lis$Wx
# It is important we estimate fixed effects parameters β using Cholesky
# decomposition. To do this, we will use the cholsolve to solve for β given
# that X^(T)WXβ = X^(T)Wy given that we have all these terms now.
XtWy <- t(X) %*% Wmul(y, qrZ, sigma2, n, p, psi, L = L)$Wx
beta <- cholsolve(t(X) %*% WX, XtWy)$x
# Calculate the first term in the log likelihood, optimising the efficiency using
# our Wmul function.
firstterm <- t(y - X %*% beta) %*% Wmul(y - X %*% beta, qrZ, sigma2, n, p, psi, L = L)$Wx
# Calculate negative log-likelihood of the model for the given θ.
loglikelihood <- -firstterm / 2 - halflogofdet
# Store fixed effects parameter estimates β as an attribute of loglikelihood.
attr(loglikelihood, "beta") <- beta
# Return loglikelihood.
return(loglikelihood)
}
## The lmm function esimates model parameters β and θ for a linear mixed model
## of the form y = Xβ + ZB + ε.
## Inputs:
## form: the formula used for the fixed effects part of the model (y ~ Xβ + ε).
## dat: the data frame containing all variables needed in the model.
## ref: the list of vectors of variable names specifying the random effects for
##      the random effects part of the model
## Outputs:
## sd: the vector of the standard deviations σ of the residuals and random effects.
## theta: the vector of log-standard deviations of the residuals and random effects.
## beta: the vector of fixed effects parameter estimates β.
lmm <- function(form, dat, ref = list()) {
# Initialise the basic components of our linear mixed model: X, y, arZ, and lengths.
setup <- LMMsetup(form, dat, ref)
X <- setup$X
y <- setup$y
qrZ <- setup$qrZ
lengths <- setup$lengths
## If there are no residual effects, default to a standard linear model.
if (is.null(qrZ)) {
# Estimate fixed effects parameters β using Cholesky decomposition.
beta <- cholsolve(t(X) %*% X, t(X) %*% y)$x
# Compute residuals ε = y - Xβ.
yxbeta <- y - X %*% beta
# Compute the sum of squared residuals.
yXBeta2 <- t(yxbeta) %*% yxbeta
# Initialise log-standard deviation of residuals as 0.
theta_init <- 0
# Set upper and lower bounds for θ optimisation.
lower <- -50
upper <- 50
# Optimise θ with respect to the NoEffectLMMprof function.
opt_result <- optim(
par = theta_init,
# Note: Negative since optim minimises.
fn = function(theta) -as.numeric(NoEffectLMMprof(theta, yXBeta2, nrow(X))),
upper = upper,
lower = lower,
method = "Brent", # Brent's method for scalar optimisation.
)
# Store the optimised θ and loglikelihood.
theta_opt <- opt_result$par
final_ll <- NoEffectLMMprof(theta_opt, yXBeta2, nrow(X))
## If there are random effects, use linear mixed model.
} else {
# Initialise θ with length number of random effects + 1 (for residuals).
theta_init <- rep(0, length(ref) + 1)
# Optimise θ with respect to the LMMprof function.
opt_result <- optim(
par = theta_init,
# Note: Negative since optim minimises.
fn = function(theta) -as.numeric(LMMprof(theta, y, X, qrZ, lengths)),
# Set precision and iteration limits.
control = list(reltol = 10^(-15), maxit = 100000)
)
# Store the optimised θ and loglikelihood.
theta_opt <- opt_result$par
final_ll <- LMMprof(theta_opt, y, X, qrZ, lengths)
# Extract beta from loglikelihood to return.
beta <- attr(final_ll, "beta")
}
# Return theta, and beta.
return(list(beta = beta, theta = theta_opt))
}
# Test the function
library(nlme)
library(lme4)
### Test Setup ###
#setup <- LMMsetup(score ~ Machine, Machines, list("Worker", c("Worker", "Machine")))
#print(setup)
### Default Test ###
result <- lmm(score ~ Machine, Machines, list("Worker", c("Worker", "Machine")))
lmer_result <- lmer(score ~ Machine + (1 | Worker) + (1 | Worker:Machine),
data = Machines,
REML = FALSE
)
lmer_result
result
### Default Empty List Test ###
result <- lmm(score ~ Machine, Machines)
lm_result <- lm(score ~ Machine,
data = Machines)
summary(lm_result)
result
## Assay Tests
result <- lmm(logDens ~ dilut + sample + Block, Assay, list(c("Block","sample")))
result
lmer_result <- lmer(logDens ~ dilut + sample + Block + (1 | Block:sample),
data = Assay,
REML = FALSE
)
lmer_result
## Meat Tests ##
result <- lmm(score ~ Storage + Block, Meat, list(c("Block", "Storage")))
result
lmer(score ~ Storage + Block + (1 | Block) + (1 | Storage) + (1 | Block:Storage),
data = Meat,
REML = FALSE
)
## Benjamin Capon - S2746256
## Tom Davies - S2210543
## Cyrus Seyrafi - S2766504
## The code was completed collaboratively mainly during workshops with all 3 team
## members consistently present and contributing ideas, pushing to Github on Tom's
## laptop (to avoid git conflicts). All team members pulled their weight by making
## further contributions to the code in their own time at home when necessary. Coding,
## commenting/editing, and breaking down the problems were done in different ratios
## within the group but the time committed to the project and overall proportion of
## contributions are seen as equal amongst group members.
## This script implements a Linear Mixed Model, given by the formula y = Xβ + Zb + ε.
## The model follows a similar format to the standard linear model (y = Xβ + ε),
## but will include the Zb term to incorporate fixed but random effects. b will
## contain the random effects, following a normal distribution with mean 0 and
## covariance matrix φ_θ parameterised by an unknown vector parameter θ, whilst Z is a
## model matrix for b. The idea behind this extra term is that we can now account
## for clustering of data or in other words, there is randomness at different levels
## within the data we can now account for.
## The LMMsetup function produces the basic components of a linear mixed model,
## represented by the formula y = Xβ + Zb + ε. N.B. the function will return
## the basic components for a linear model by default if no ref is supplied.
##
## Inputs:
##  form: the formula used for the fixed effects part of the model (y ~ Xβ + ε).
##  dat: the data frame containing all variables needed in the model.
##  ref: the list of vectors of variable names identifying the random effects part
##       of the model.
##
## Outputs:
##  X: the design matrix for fixed effects.
##  y: the response vector.
##  qrZ: the QR decomposition of random effects design matrix Z.
##  lengths: the number of columns in each fixed effects block
LMMsetup <- function(form, dat, ref = list()) {
# Construct fixed effects design matrix X.
X <- model.matrix(form, dat)
# Extract response vector y from the model data frame. Store as 1 column matrix
# for convenience in calculations and operations.
y <- model.response(model.frame(form, dat))
y <- matrix(y, ncol = 1)
# If no random effects, return X and y for linear model as there will be no Zb.
if (length(ref) == 0) {
return(list(X = X, y = y, qrZ = NULL, lengths = 0))
} else {
# Otherwise, initialise empty random effects design matrix.
Z <- matrix(NA, nrow(dat), 0)
}
# Also initialise a lengths vector where for each random effect vector in ref,
# we add a block to Z.
lengths <- rep(0, length(ref))
# Iterate through each random effects vector in ref.
for (i in 1:length(ref)) {
# Construct a string for the linear model formula, and parse it with as.formula.
# e.g. c("a", "b", "c") -> "~ a:b:c-1" -> ~ a:b:c-1
# The "-1" allows us to exclude the intercept from our effect formula.
effectformula <- as.formula(paste(c("~ ", paste(ref[[i]], collapse = ":"), "-1"), collapse = ""))
# Assemble this block of the random effects design matrix Z.
block <- model.matrix(effectformula, dat)
# Assign the number of columns in the block to the lengths vector.
lengths[i] <- ncol(block)
# Bind this block to random effects design matrix Z.
Z <- cbind(Z, block)
}
# Determine dimensions of random effects design matrix Z (an n x p matrix).
n <- length(y)
p <- sum(lengths)
# If p > n, stop since n must be larger than p in our assumptions. Print an
# error message and stop the code running.
if(p >= n){
stop("n must be larger than p!")
}
# Finally, take the qr decomposition of Z and return it in a list alongside
# all the other useful objects that we can avoid repeating calculations on
# when fitting a linear mixed model.
qrZ <- qr(Z)
return(list(X = X, y = y, qrZ = qrZ, lengths = lengths))
}
## The cholsolve function solves a system of the form Ax = y using Cholesky decomposition.
## This method, while still O(n**3) time if we have to perform the decomposition,
## has a smaller constant of proportionality than solve, and is thus more optimal.
##
## Inputs:
##  A: a symmetric positive definite matrix.
##  y: the vector of response variables.
##  R: an optional Cholesky factor used representing A the upper triangular Cholesky
##     factor of the decomposition (A = R^(T)R) if it is available.
##
## Outputs:
##  x: the vector which is to be solved for (such that Ax = y).
##  R: the upper triangular Cholesky factor, for future use.
cholsolve <- function(A, y, R = NULL) {
# Unless provided, calculate upper-triangular Cholesky factor R such that A = R^(T)R.
if (is.null(R)) {
R <- chol(A)
}
# Solve R^(T)z = y for z, and then Rx = z for x, using efficient triangular matrix methods.
x <- backsolve(R, forwardsolve(t(R), y))
# Return vector x and Cholesky factor R.
return(list(x = x, R = R))
}
## Here we write a function, Wmul, which will perform the calculation related to
## multiplications with W. W is given by ZψZ^(T) + Iσ**2 and is a part of both
## terms in the log likelihood for theta and beta.
##
## Inputs:
##  A: a symmetric positive definite matrix.
##  y: the vector of response variables.
##  L: an optional Cholesky factor used representing A in Cholesky decomposition
##     (so A = L^(T)L).
##
## Outputs:
##  x: the vector which is to be solved for (such that Ax = y).
##  L: the Cholesky factor, for future use.
Wmul <- function(x, qrZ, sigma2, n, p, phi, L = NULL) {
# Compute (Q^T)x by using qr.qty() which multiplies x by the transpose of Q
Qtx <- qr.qty(qrZ, x)
# Check if we haven't already performed a Cholesky decomposition to get the upper
# triangular factor and if not, ?
if (is.null(L)) {
WXL <- cholsolve(qr.R(qrZ) %*% phi %*% t(qr.R(qrZ)) + diag(sigma2, p), Qtx)
} else {
WXL <- cholsolve(NULL, Qtx, L = L)
}
# Initialize full n × ncol(X) matrix
Wx <- matrix(0, n, ncol(x))
# Fill in first p rows from solution
Wx[1:p, ] <- WXL$x
# Fill in remaining rows
Wx[(p + 1):n, ] <- Qtx[(p + 1):n, ] / sigma2
Wx <- qr.qy(qrZ, Wx)
return(list(Wx = Wx, L = WXL$L))
}
## Here we write a function, NoEffectLMMprof, to calculate the log likelihood given
## there are no random effects. This is equivalent to calculating the log likelihood
## of a normal linear regression model.
##
## Inputs:
##  theta: the log of the standard deviation
##  yXbeta2: the evaluated (y-Xβ)^T(y-Xβ) to avoid repetition
##  n: the number of rows of data we have
##
## Outputs:
##  loglik: the log likelihood, excluding constant terms
NoEffectLMMprof <- function(theta, yXBeta2, n) {
# theta is the log of standard deviation so perform the inverse to get sigma**2:
sigma2 <- exp(2 * theta)
# The log likelihood of the normal linear regression model is - (y-Xβ)^T(y-Xβ)/2σ**2
# - (n/2)ln(σ**2) - (n/2)ln(2π). The last term can be dropped as it's a constant
# so calculate this as loglik and return it:
loglik <- -yXBeta2 / (2 * sigma2) - n * log(sigma2) / 2
return(loglik)
}
## The function LMMprof evaluates the negative log-likelihood of the model for a
## given set of log standard deviations (the vector θ), and stores the corresponding
## estimates of the fixed effects parameters β as an attribute.
##
## Inputs:
## theta: the vector of log-standard deviations of the residuals and random effects.
## y: the response vector.
## X: the design matrix for fixed effects.
## qrZ: the QR decomposition of the random effects design matrix Z.
## lengths: number of random effects variables in each entry of ref.
##
## Outputs:
## loglikelihood: the negative log-likelihood of the model for the given θ, with
## beta: estimate β stored as an attribute.
LMMprof <- function(theta, y, X, qrZ, lengths) {
# Compute the dimensions of n x p random effects design matrix Z.
p <- sum(lengths)
n <- length(y)
# Calculate the variance of the residuals (σ**2) from theta[1] = log(σ).
sigma2 <- exp(2 * theta[1])
# Construct random effects covariance matrix ψ from remaining values in θ where
# ψ is a diagonal matrix; random effects are independent.
# N.B. theta[-1] are the log-standard deviations of the random effects.
psi <- diag(rep(exp(2 * theta[-1]), lengths))
# Combine random effects covariance matrix ψ and residual variance to
# construct reduced covariance matrix of model.
mat2log <- qr.R(qrZ) %*% psi %*% t(qr.R(qrZ)) + diag(sigma2, p)
# Compute Cholesky factorisation of covariance matrix (R^(T)R = S).
MatChol <- chol(mat2log)
# Compute half the logarithm of the determinant of the reduced covariance matrix.
halflogofdet <- sum(log(diag(MatChol))) + (n - p) * log(sigma2) / 2
# Compute W-weighted version of design matrix X which has useful attributes we
# can use.
lis <- Wmul(X, qrZ, sigma2, n, p, psi)
# Extract Cholesky factor L for reuse.
L <- lis$L
# Extract the crossproduct between W and x using our lis variable.
WX <- lis$Wx
# It is important we estimate fixed effects parameters β using Cholesky
# decomposition. To do this, we will use the cholsolve to solve for β given
# that X^(T)WXβ = X^(T)Wy given that we have all these terms now.
XtWy <- t(X) %*% Wmul(y, qrZ, sigma2, n, p, psi, L = L)$Wx
beta <- cholsolve(t(X) %*% WX, XtWy)$x
# Calculate the first term in the log likelihood, optimising the efficiency using
# our Wmul function.
firstterm <- t(y - X %*% beta) %*% Wmul(y - X %*% beta, qrZ, sigma2, n, p, psi, L = L)$Wx
# Calculate negative log-likelihood of the model for the given θ.
loglikelihood <- -firstterm / 2 - halflogofdet
# Store fixed effects parameter estimates β as an attribute of loglikelihood.
attr(loglikelihood, "beta") <- beta
# Return loglikelihood.
return(loglikelihood)
}
## The lmm function esimates model parameters β and θ for a linear mixed model
## of the form y = Xβ + ZB + ε.
##
## Inputs:
## form: the formula used for the fixed effects part of the model (y ~ Xβ + ε).
## dat: the data frame containing all variables needed in the model.
## ref: the list of vectors of variable names specifying the random effects for
##      the random effects part of the model
##
## Outputs:
## sd: the vector of the standard deviations σ of the residuals and random effects.
## theta: the vector of log-standard deviations of the residuals and random effects.
## beta: the vector of fixed effects parameter estimates β.
lmm <- function(form, dat, ref = list()) {
# Initialise the basic components of our linear mixed model: X, y, arZ, and lengths.
setup <- LMMsetup(form, dat, ref)
X <- setup$X
y <- setup$y
qrZ <- setup$qrZ
lengths <- setup$lengths
## If there are no residual effects, default to a standard linear model.
if (is.null(qrZ)) {
# Estimate fixed effects parameters β using Cholesky decomposition.
beta <- cholsolve(t(X) %*% X, t(X) %*% y)$x
# Compute residuals ε = y - Xβ.
yxbeta <- y - X %*% beta
# Compute the sum of squared residuals.
yXBeta2 <- t(yxbeta) %*% yxbeta
# Initialise log-standard deviation of residuals as 0.
theta_init <- 0
# Set upper and lower bounds for θ optimisation.
lower <- -50
upper <- 50
# Optimise θ with respect to the NoEffectLMMprof function.
opt_result <- optim(
par = theta_init,
# Note: Negative since optim minimises.
fn = function(theta) -as.numeric(NoEffectLMMprof(theta, yXBeta2, nrow(X))),
upper = upper,
lower = lower,
method = "Brent", # Brent's method for scalar optimisation.
)
# Store the optimised θ and loglikelihood.
theta_opt <- opt_result$par
final_ll <- NoEffectLMMprof(theta_opt, yXBeta2, nrow(X))
## If there are random effects, use linear mixed model.
} else {
# Initialise θ with length number of random effects + 1 (for residuals).
theta_init <- rep(0, length(ref) + 1)
# Optimise θ with respect to the LMMprof function.
opt_result <- optim(
par = theta_init,
# Note: Negative since optim minimises.
fn = function(theta) -as.numeric(LMMprof(theta, y, X, qrZ, lengths)),
# Set precision and iteration limits.
control = list(reltol = 10^(-15), maxit = 100000)
)
# Store the optimised θ and loglikelihood.
theta_opt <- opt_result$par
final_ll <- LMMprof(theta_opt, y, X, qrZ, lengths)
# Extract beta from loglikelihood to return.
beta <- attr(final_ll, "beta")
}
# Return theta, and beta.
return(list(beta = beta, theta = theta_opt))
}
library(rjags)
help(dcat)
help(dpois)
y <- c(12,14,33,50,67,74,123,141,165,204,253,246,240)
t <- -6:6 ## year zero is 1987
#setwd() ## EDIT: where is JAGS file?
jaid <- jags.model("model.jags",data=list(t=t,y=y))
y <- c(12,14,33,50,67,74,123,141,165,204,253,246,240)
t <- -6:6 ## year zero is 1987
#setwd() ## EDIT: where is JAGS file?
jaid <- jags.model("model.jags",data=list(t=t,y=y))
library(rjags)
install.packages(rjags)
install.packages("rjags")
library(rjags)
library(rjags)
R --version
install.packages("rjags")
library(rjags)
install.packages("installr")
library(installr)
updateR()
library(installr)
updateR()
library(rjags)
library(rjags)
library(rjags)
install.packages("rjags")
library("rjags")
install.packages("coda")
library("rjags")
library("rjags")
y <- c(12,14,33,50,67,74,123,141,165,204,253,246,240)
t <- -6:6 ## year zero is 1987
#setwd() ## EDIT: where is JAGS file?
jaid <- jags.model("model.jags",data=list(t=t,y=y))
library("rjags")
y <- c(12,14,33,50,67,74,123,141,165,204,253,246,240)
t <- -6:6 ## year zero is 1987
#setwd() ## EDIT: where is JAGS file?
jaid <- jags.model("model.jags",data=list(t=t,y=y))
library("rjags")
y <- c(12,14,33,50,67,74,123,141,165,204,253,246,240)
t <- -6:6 ## year zero is 1987
setwd("C:/Users/BCapo/Documents/GitHub/extended-statistical-programming/RJAGS") ## EDIT: where is JAGS file?
jaid <- jags.model("model.jags",data=list(t=t,y=y))
library("rjags")
y <- c(12,14,33,50,67,74,123,141,165,204,253,246,240)
t <- -6:6 ## year zero is 1987
setwd("C:/Users/BCapo/Documents/GitHub/extended-statistical-programming/RJAGS") ## EDIT: where is JAGS file?
jaid <- jags.model("model.jags",data=list(t=t,y=y))
library("rjags")
y <- c(12,14,33,50,67,74,123,141,165,204,253,246,240)
t <- -6:6 ## year zero is 1987
setwd("C:/Users/BCapo/Documents/GitHub/extended-statistical-programming/RJAGS") ## EDIT: where is JAGS file?
jaid <- jags.model("model.jags",data=list(t=t,y=y))
library("rjags")
y <- c(12,14,33,50,67,74,123,141,165,204,253,246,240)
t <- -6:6 ## year zero is 1987
setwd("C:/Users/BCapo/Documents/GitHub/extended-statistical-programming/RJAGS") ## EDIT: where is JAGS file?
jaid <- jags.model("model.jags",data=list(t=t,y=y))
sam <- coda.samples(jaid,c("th","mu"),n.iter=20000)
library("rjags")
y <- c(12,14,33,50,67,74,123,141,165,204,253,246,240)
t <- -6:6 ## year zero is 1987
setwd("C:/Users/BCapo/Documents/GitHub/extended-statistical-programming/RJAGS") ## EDIT: where is JAGS file?
jaid <- jags.model("model.jags",data=list(t=t,y=y))
sam <- coda.samples(jaid,c("th","mu"),n.iter=20000)
library("rjags")
y <- c(12,14,33,50,67,74,123,141,165,204,253,246,240)
t <- -6:6 ## year zero is 1987
setwd("C:/Users/BCapo/Documents/GitHub/extended-statistical-programming/RJAGS") ## EDIT: where is JAGS file?
jaid <- jags.model("model.jags",data=list(t=t,y=y))
sam <- coda.samples(jaid,c("theta","mu"),n.iter=20000)
str(sam)
crosscorr(sam)
crosscorr(sam.coda)
HPDinterval(sam.coda)
HPDinterval(sam)
effectiveSize(sam)
